"""
Fine-tuning Script for 3D Segmentation
‚úÖ FIXED: No hard-coded values (all from config)
‚úÖ FIXED: Robust model loading (no fallback configs)
‚úÖ FIXED: 3D-native augmentation (torchio)
‚úÖ FIXED: Proper data split handling
‚úÖ FIXED: Configurable unfreeze LR
‚úÖ FIXED: Mixed precision training support
‚úÖ FIXED: Comprehensive logging

File: scripts/finetune.py
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
import argparse
from pathlib import Path
import yaml
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
import logging
from typing import Dict, Optional
import json

import sys
sys.path.append('..')

from models import HGFormer3D, HGFormer3D_ForSegmentation
from models.losses import segmentation_loss
from utils.data_loader import SELMA3DDataset
from utils.augmentations import get_finetune_transforms, get_val_transforms
from utils.metrics import dice_coefficient, iou_score
import torchio as tio

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def parse_args():
    parser = argparse.ArgumentParser(
        description='Fine-tune HGFormer3D for 3D segmentation',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument('--config', type=str, required=True,
                       help='Path to finetune config YAML file')
    parser.add_argument('--pretrained_checkpoint', type=str, default=None,
                       help='Path to pretrained checkpoint (overrides config)')
    parser.add_argument('--output_dir', type=str, default=None,
                       help='Output directory (overrides config)')
    parser.add_argument('--gpu', type=int, default=0,
                       help='GPU device ID')
    parser.add_argument('--resume', type=str, default=None,
                       help='Path to checkpoint to resume from')
    parser.add_argument('--debug', action='store_true',
                       help='Enable debug logging')
    return parser.parse_args()


def load_pretrained_encoder(checkpoint_path: str, device: torch.device) -> HGFormer3D:
    """
    ‚úÖ FIXED: Robust model loading with validation
    
    Args:
        checkpoint_path: Path to SSL checkpoint
        device: Device to load model on
        
    Returns:
        HGFormer3D encoder with pretrained weights
        
    Raises:
        FileNotFoundError: If checkpoint doesn't exist
        ValueError: If checkpoint is missing config
        RuntimeError: If weights don't match architecture
    """
    checkpoint_path = Path(checkpoint_path)
    
    # Validate checkpoint exists
    if not checkpoint_path.exists():
        raise FileNotFoundError(
            f"Pretrained checkpoint not found: {checkpoint_path}\n"
            f"Did you run SSL pretraining? Expected file at:\n"
            f"  {checkpoint_path.absolute()}"
        )
    
    logger.info(f"Loading pretrained encoder from: {checkpoint_path}")
    
    # Load checkpoint
    try:
        checkpoint = torch.load(checkpoint_path, map_location=device)
    except Exception as e:
        raise RuntimeError(f"Failed to load checkpoint: {e}")
    
    # ‚úÖ CRITICAL FIX: Enforce config presence (NO FALLBACKS)
    if 'config' not in checkpoint:
        raise ValueError(
            f"CRITICAL ERROR: Checkpoint is missing 'config' key.\n"
            f"This checkpoint was created with an older version of pretrain.py.\n"
            f"Solution: Re-run SSL pretraining to generate a valid checkpoint.\n"
            f"Checkpoint path: {checkpoint_path}"
        )
    
    # Extract model config
    model_config = checkpoint['config']['model']
    
    logger.info("Pretrained model configuration:")
    for key, value in model_config.items():
        logger.info(f"  {key}: {value}")
    
    # Create encoder with exact config from checkpoint
    try:
        encoder = HGFormer3D(**model_config)
    except Exception as e:
        raise RuntimeError(
            f"Failed to create encoder with config from checkpoint:\n"
            f"Config: {model_config}\n"
            f"Error: {e}"
        )
    
    # Load weights
    try:
        encoder.load_state_dict(checkpoint['model_state_dict'])
    except Exception as e:
        raise RuntimeError(
            f"Failed to load pretrained weights.\n"
            f"This usually means the checkpoint config doesn't match the model architecture.\n"
            f"Error: {e}"
        )
    
    logger.info("‚úÖ Pretrained weights loaded successfully")
    
    # Log training info if available
    if 'epoch' in checkpoint:
        logger.info(f"  Pretrained for {checkpoint['epoch']} epochs")
    if 'loss' in checkpoint:
        logger.info(f"  Final SSL loss: {checkpoint['loss']:.4f}")
    
    return encoder


def create_dataloaders(config: Dict, device: torch.device) -> tuple:
    """
    ‚úÖ FIXED: Simplified data loading (no confusing split logic)
    
    Args:
        config: Configuration dictionary
        device: Device for pin_memory optimization
        
    Returns:
        train_loader, val_loader
    """
    logger.info("\nCreating dataloaders...")
    
    # Get config
    aug_config = config['data'].get('augmentation', {})
    use_augmentation = config['data'].get('use_augmentation', True)
    patch_size = tuple(config['data'].get('patch_size', [64, 128, 128]))
    samples_per_volume = config['data'].get('samples_per_volume', 10)
    
    # Create transforms
    if use_augmentation:
        train_transform = get_finetune_transforms(aug_config)
        logger.info("  Augmentation: ENABLED")
    else:
        train_transform = get_val_transforms()
        logger.info("  Augmentation: DISABLED")
    
    val_transform = get_val_transforms()
    
    # ‚úÖ FIXED: Pass patch_size to dataset
    train_dataset = SELMA3DDataset(
        data_dir=config['data']['train_dir'],
        patch_size=patch_size,
        samples_per_volume=samples_per_volume,
        transform=train_transform
    )
    
    val_dataset = SELMA3DDataset(
        data_dir=config['data']['val_dir'],
        patch_size=patch_size,
        samples_per_volume=5,  # Fewer patches for faster validation
        transform=val_transform
    )
    
    logger.info(f"  Train dataset: {len(train_dataset)} patches")
    logger.info(f"  Val dataset: {len(val_dataset)} patches")
    
    # Validate dataset sizes
    if len(train_dataset) == 0:
        raise ValueError(f"Train dataset is empty! Check path: {config['data']['train_dir']}")
    if len(val_dataset) == 0:
        raise ValueError(f"Val dataset is empty! Check path: {config['data']['val_dir']}")
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['training']['batch_size'],
        shuffle=True,
        num_workers=config['data']['num_workers'],
        pin_memory=config['data']['pin_memory'] and device.type == 'cuda',
        drop_last=True,
        prefetch_factor=config['data'].get('prefetch_factor', 2)
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=1,  # Validate one at a time for memory efficiency
        shuffle=False,
        num_workers=2,
        pin_memory=config['data']['pin_memory'] and device.type == 'cuda'
    )
    
    logger.info(f"  Train batches: {len(train_loader)}")
    logger.info(f"  Val batches: {len(val_loader)}")
    
    return train_loader, val_loader


def validate(
    model: nn.Module,
    val_loader: DataLoader,
    device: torch.device,
    epoch: int
) -> Dict[str, float]:
    """
    Compute validation metrics
    
    Args:
        model: Segmentation model
        val_loader: Validation data loader
        device: Device to run on
        epoch: Current epoch (for logging)
        
    Returns:
        Dict with 'dice', 'iou', 'dice_std', 'iou_std'
    """
    model.eval()
    
    dice_scores = []
    iou_scores = []
    
    with torch.no_grad():
        for volumes, masks, metadata in tqdm(val_loader, desc='Validation', leave=False):
            volumes = volumes.to(device)
            masks = masks.to(device)
            
            # Forward pass
            try:
                logits = model(volumes)
            except Exception as e:
                logger.error(f"Validation forward pass failed: {e}")
                logger.error(f"  Volume shape: {volumes.shape}")
                raise
            
            # Get predictions
            preds = torch.argmax(logits, dim=1)
            
            # Compute metrics for each sample in batch
            for i in range(preds.shape[0]):
                pred_np = preds[i].cpu().numpy()
                mask_np = masks[i].cpu().numpy()
                
                dice = dice_coefficient(pred_np, mask_np)
                iou = iou_score(pred_np, mask_np)
                
                dice_scores.append(dice)
                iou_scores.append(iou)
    
    # Compute statistics
    metrics = {
        'dice': float(np.mean(dice_scores)),
        'iou': float(np.mean(iou_scores)),
        'dice_std': float(np.std(dice_scores)),
        'iou_std': float(np.std(iou_scores))
    }
    
    return metrics


def save_predictions(
    model: nn.Module,
    val_loader: DataLoader,
    device: torch.device,
    output_dir: Path,
    epoch: int,
    num_samples: int = 4
):
    """
    Save visualization of predictions
    
    Args:
        model: Segmentation model
        val_loader: Validation data loader
        device: Device to run on
        output_dir: Output directory
        epoch: Current epoch
        num_samples: Number of samples to visualize
    """
    model.eval()
    
    vis_dir = output_dir / 'visualizations' / f'epoch_{epoch:04d}'
    vis_dir.mkdir(parents=True, exist_ok=True)
    
    samples_saved = 0
    
    with torch.no_grad():
        for volumes, masks, metadata in val_loader:
            if samples_saved >= num_samples:
                break
            
            volumes = volumes.to(device)
            
            # Forward pass
            logits = model(volumes)
            preds = torch.argmax(logits, dim=1)
            
            # Get probabilities for visualization
            probs = torch.softmax(logits, dim=1)
            
            # Save visualizations
            for i in range(min(volumes.shape[0], num_samples - samples_saved)):
                volume = volumes[i, 0].cpu().numpy()
                mask = masks[i].cpu().numpy()
                pred = preds[i].cpu().numpy()
                prob_fg = probs[i, 1].cpu().numpy()  # Foreground probability
                
                # Take middle slice
                mid_slice = volume.shape[0] // 2
                
                # Create figure with 4 subplots
                fig, axes = plt.subplots(1, 4, figsize=(20, 5))
                
                # Input
                axes[0].imshow(volume[mid_slice], cmap='gray')
                axes[0].set_title('Input', fontsize=14)
                axes[0].axis('off')
                
                # Ground Truth
                axes[1].imshow(mask[mid_slice], cmap='jet', vmin=0, vmax=1)
                axes[1].set_title('Ground Truth', fontsize=14)
                axes[1].axis('off')
                
                # Prediction
                axes[2].imshow(pred[mid_slice], cmap='jet', vmin=0, vmax=1)
                axes[2].set_title('Prediction', fontsize=14)
                axes[2].axis('off')
                
                # Probability Map
                im = axes[3].imshow(prob_fg[mid_slice], cmap='hot', vmin=0, vmax=1)
                axes[3].set_title('Foreground Probability', fontsize=14)
                axes[3].axis('off')
                plt.colorbar(im, ax=axes[3], fraction=0.046)
                
                # Add metrics as suptitle
                dice = dice_coefficient(pred, mask)
                iou = iou_score(pred, mask)
                fig.suptitle(
                    f"Sample {samples_saved} - Dice: {dice:.4f}, IoU: {iou:.4f}",
                    fontsize=16
                )
                
                plt.tight_layout()
                plt.savefig(
                    vis_dir / f'sample_{samples_saved:03d}.png',
                    bbox_inches='tight',
                    dpi=150
                )
                plt.close()
                
                samples_saved += 1
    
    logger.info(f"  üíæ Saved {samples_saved} visualizations to {vis_dir}")


def main():
    args = parse_args()
    
    # Set logging level
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Load config
    logger.info(f"Loading config from {args.config}")
    with open(args.config) as f:
        config = yaml.safe_load(f)
    
    # Override config with command line args
    if args.pretrained_checkpoint:
        config['pretrained_checkpoint'] = args.pretrained_checkpoint
    if args.output_dir:
        config['paths']['output_dir'] = args.output_dir
    
    # Create output directories
    output_dir = Path(config['paths']['output_dir'])
    log_dir = Path(config['paths']['log_dir'])
    output_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # Save config to output dir
    config_save_path = output_dir / 'finetune_config.yaml'
    with open(config_save_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False, sort_keys=False)
    logger.info(f"Config saved to: {config_save_path}")
    
    # Setup device
    device = torch.device(f'cuda:{args.gpu}' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Using device: {device}")
    
    if device.type == 'cuda':
        logger.info(f"  GPU: {torch.cuda.get_device_name(args.gpu)}")
        logger.info(f"  Memory: {torch.cuda.get_device_properties(args.gpu).total_memory / 1e9:.2f} GB")
    
    # Load pretrained encoder
    encoder = load_pretrained_encoder(config['pretrained_checkpoint'], device)
    
    # Create segmentation model
    logger.info("\nCreating segmentation model...")
    model = HGFormer3D_ForSegmentation(
        pretrained_encoder=encoder,
        num_classes=config['model']['num_classes'],
        freeze_encoder=config['model']['freeze_encoder'],
        decoder_channels=config['decoder']['decoder_channels'],
        use_attention=config['decoder']['use_attention']
    ).to(device)
    
    # Count parameters
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    frozen_params = total_params - trainable_params
    
    logger.info(f"Model created:")
    logger.info(f"  Total parameters: {total_params/1e6:.2f}M")
    logger.info(f"  Trainable parameters: {trainable_params/1e6:.2f}M")
    logger.info(f"  Frozen parameters: {frozen_params/1e6:.2f}M")
    logger.info(f"  Encoder frozen: {config['model']['freeze_encoder']}")
    
    # Create dataloaders
    train_loader, val_loader = create_dataloaders(config, device)
    
    # Setup optimizer (only trainable parameters)
    logger.info("\nSetting up optimizer...")
    trainable_params_list = [p for p in model.parameters() if p.requires_grad]
    
    optimizer = torch.optim.AdamW(
        trainable_params_list,
        lr=config['training']['learning_rate'],
        weight_decay=config['training']['weight_decay']
    )
    
    logger.info(f"  Optimizer: AdamW")
    logger.info(f"  Learning rate: {config['training']['learning_rate']}")
    logger.info(f"  Weight decay: {config['training']['weight_decay']}")
    
    # Setup scheduler
    scheduler_type = config['training']['scheduler']
    
    if scheduler_type == 'ReduceLROnPlateau':
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='max',
            factor=config['training']['lr_factor'],
            patience=config['training']['patience'],
            verbose=True,
            min_lr=config['training']['min_lr']
        )
        logger.info(f"  Scheduler: ReduceLROnPlateau")
        logger.info(f"    Patience: {config['training']['patience']}")
        logger.info(f"    Factor: {config['training']['lr_factor']}")
    
    elif scheduler_type == 'CosineAnnealing':
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=config['training']['epochs'],
            eta_min=config['training']['min_lr']
        )
        logger.info(f"  Scheduler: CosineAnnealingLR")
        logger.info(f"    T_max: {config['training']['epochs']}")
    
    else:
        raise ValueError(f"Unknown scheduler type: {scheduler_type}")
    
    # Setup mixed precision training (if enabled)
    use_amp = config.get('amp', {}).get('enabled', False)
    if use_amp:
        scaler = GradScaler()
        logger.info("  Mixed precision: ENABLED")
    else:
        scaler = None
        logger.info("  Mixed precision: DISABLED")
    
    # Resume from checkpoint if provided
    start_epoch = 0
    best_dice = 0.0
    patience_counter = 0
    training_history = []
    
    if args.resume:
        logger.info(f"\nResuming from checkpoint: {args.resume}")
        checkpoint = torch.load(args.resume, map_location=device)
        
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if 'scheduler_state_dict' in checkpoint:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        start_epoch = checkpoint['epoch'] + 1
        best_dice = checkpoint.get('best_dice', 0.0)
        
        if 'training_history' in checkpoint:
            training_history = checkpoint['training_history']
        
        logger.info(f"  Resuming from epoch {start_epoch}")
        logger.info(f"  Best Dice so far: {best_dice:.4f}")
    
    # Training loop
    logger.info("\n" + "="*80)
    logger.info("STARTING FINE-TUNING")
    logger.info("="*80)
    logger.info(f"Total epochs: {config['training']['epochs']}")
    logger.info(f"Batch size: {config['training']['batch_size']}")
    logger.info(f"Validation frequency: every {config['training']['val_freq']} epoch(s)")
    logger.info(f"Checkpoint frequency: every {config['training']['save_freq']} epoch(s)")
    
    if config['model']['freeze_encoder']:
        logger.info(f"Encoder will unfreeze at epoch {config['model']['unfreeze_epoch']}")
    
    logger.info("="*80 + "\n")
    
    try:
        for epoch in range(start_epoch, config['training']['epochs']):
            
            # ‚úÖ FIXED: Configurable unfreeze LR
            if epoch == config['model']['unfreeze_epoch'] and config['model']['freeze_encoder']:
                logger.info(f"\n{'='*80}")
                logger.info(f"üîì UNFREEZING ENCODER at epoch {epoch}")
                logger.info(f"{'='*80}")
                
                model.unfreeze_encoder()
                
                # ‚úÖ CRITICAL FIX: Use configurable LR factor
                unfreeze_lr = (
                    config['training']['learning_rate'] * 
                    config['training'].get('unfreeze_lr_factor', 0.1)
                )
                
                # Update optimizer with all parameters
                optimizer = torch.optim.AdamW(
                    model.parameters(),
                    lr=unfreeze_lr,
                    weight_decay=config['training']['weight_decay']
                )
                
                # Reset scheduler
                if scheduler_type == 'ReduceLROnPlateau':
                    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                        optimizer,
                        mode='max',
                        factor=config['training']['lr_factor'],
                        patience=config['training']['patience'],
                        verbose=True,
                        min_lr=config['training']['min_lr']
                    )
                elif scheduler_type == 'CosineAnnealing':
                    # Continue from current point
                    remaining_epochs = config['training']['epochs'] - epoch
                    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                        optimizer,
                        T_max=remaining_epochs,
                        eta_min=config['training']['min_lr']
                    )
                
                logger.info(f"  New learning rate: {unfreeze_lr}")
                logger.info(f"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6:.2f}M")
                logger.info("="*80 + "\n")
            
            # Training
            model.train()
            train_losses = {
                'total': [],
                'dice': [],
                'focal': [],
                'boundary': [],
                'alpha': []
            }
            
            pbar = tqdm(
                train_loader,
                desc=f'Epoch {epoch+1}/{config["training"]["epochs"]}',
                ncols=100
            )
            
            for batch_idx, (volumes, masks, metadata) in enumerate(pbar):
                volumes = volumes.to(device)
                masks = masks.to(device)
                
                # Forward pass with optional mixed precision
                if use_amp:
                    with autocast():
                        logits = model(volumes)
                        loss, loss_dict = segmentation_loss(
                            logits, masks,
                            alpha_start=config['loss']['alpha_start'],
                            epoch=epoch,
                            max_epochs=config['training']['epochs'],
                            loss_config=config['loss']
                        )
                else:
                    logits = model(volumes)
                    loss, loss_dict = segmentation_loss(
                        logits, masks,
                        alpha_start=config['loss']['alpha_start'],
                        epoch=epoch,
                        max_epochs=config['training']['epochs'],
                        loss_config=config['loss']
                    )
                
                # Backward pass
                optimizer.zero_grad()
                
                if use_amp:
                    scaler.scale(loss).backward()
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        model.parameters(),
                        config['training']['gradient_clip']
                    )
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(
                        model.parameters(),
                        config['training']['gradient_clip']
                    )
                    optimizer.step()
                
                # Log losses
                for key in train_losses.keys():
                    if key in loss_dict:
                        train_losses[key].append(loss_dict[key])
                
                # Update progress bar
                pbar.set_postfix({
                    'loss': f"{loss.item():.4f}",
                    'dice_loss': f"{loss_dict['dice']:.4f}",
                    'alpha': f"{loss_dict.get('alpha', 1.0):.3f}"
                })
            
            # Calculate average losses
            avg_train_losses = {
                k: np.mean(v) if len(v) > 0 else 0.0 
                for k, v in train_losses.items()
            }
            
            # Validation
            val_metrics = None
            if (epoch + 1) % config['training']['val_freq'] == 0:
                logger.info("\n  üîç Running validation...")
                val_metrics = validate(model, val_loader, device, epoch)
                
                logger.info(f"\n  üìä Epoch {epoch+1} Summary:")
                logger.info(f"    Train Loss: {avg_train_losses['total']:.4f}")
                logger.info(f"    Train Dice Loss: {avg_train_losses['dice']:.4f}")
                logger.info(f"    Loss Alpha: {avg_train_losses.get('alpha', 1.0):.3f}")
                logger.info(f"    Val Dice: {val_metrics['dice']:.4f} ¬± {val_metrics['dice_std']:.4f}")
                logger.info(f"    Val IoU: {val_metrics['iou']:.4f} ¬± {val_metrics['iou_std']:.4f}")
                logger.info(f"    Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
                
                # Step scheduler
                if scheduler_type == 'ReduceLROnPlateau':
                    scheduler.step(val_metrics['dice'])
                else:
                    scheduler.step()
                
                # Save training history
                history_entry = {
                    'epoch': epoch + 1,
                    'train_loss': avg_train_losses['total'],
                    'train_dice_loss': avg_train_losses['dice'],
                    'val_dice': val_metrics['dice'],
                    'val_iou': val_metrics['iou'],
                    'lr': optimizer.param_groups[0]['lr']
                }
                training_history.append(history_entry)
                
                # Save checkpoint
                if (epoch + 1) % config['training']['save_freq'] == 0:
                    checkpoint_path = output_dir / f'checkpoint_epoch_{epoch+1:04d}.pth'
                    torch.save({
                        'epoch': epoch,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'train_loss': avg_train_losses['total'],
                        'val_dice': val_metrics['dice'],
                        'best_dice': best_dice,
                        'config': config,
                        'training_history': training_history
                    }, checkpoint_path)
                    logger.info(f"    üíæ Checkpoint saved: {checkpoint_path.name}")
                
                # Save best model
                if val_metrics['dice'] > best_dice:
                    best_dice = val_metrics['dice']
                    patience_counter = 0
                    
                    best_path = output_dir / 'best_model.pth'
                    torch.save({
                        'epoch': epoch,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'val_dice': best_dice,
                        'config': config,
                        'training_history': training_history
                    }, best_path)
                    logger.info(f"    ‚úÖ NEW BEST MODEL! Dice: {best_dice:.4f}")
                    
                    # Save visualizations for best model
                    if config['visualization']['save_predictions']:
                        save_predictions(
                            model, val_loader, device, output_dir, epoch,
                            num_samples=config['visualization']['num_samples']
                        )
                else:
                    patience_counter += 1
                    logger.info(
                        f"    ‚è≥ No improvement. Patience: "
                        f"{patience_counter}/{config['training']['early_stopping_patience']}"
                    )
                
                # Early stopping
                if config['training']['early_stopping']:
                    if patience_counter >= config['training']['early_stopping_patience']:
                        logger.info(f"\n  üõë Early stopping triggered after {epoch + 1} epochs")
                        break
            
            logger.info("")  # Add newline for readability
    
    except KeyboardInterrupt:
        logger.info("\n\n‚ö†Ô∏è  Training interrupted by user")
        logger.info("Saving checkpoint before exit...")
        
        # Save interrupted checkpoint
        interrupt_path = output_dir / 'interrupted_checkpoint.pth'
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'best_dice': best_dice,
            'config': config,
            'training_history': training_history
        }, interrupt_path)
        logger.info(f"Checkpoint saved: {interrupt_path}")
    
    except Exception as e:
        logger.error(f"\n\n‚ùå Training failed with error: {e}")
        logger.exception("Full traceback:")
        raise
    
    finally:
        # Save training history
        history_path = output_dir / 'training_history.json'
        with open(history_path, 'w') as f:
            json.dump(training_history, f, indent=2)
        logger.info(f"\nüìä Training history saved: {history_path}")
    
    # Final summary
    logger.info("\n" + "="*80)
    logger.info("FINE-TUNING COMPLETED!")
    logger.info("="*80)
    logger.info(f"Best validation Dice: {best_dice:.4f}")
    logger.info(f"Total epochs: {epoch + 1}")
    logger.info(f"Checkpoints saved to: {output_dir}")
    logger.info(f"Training history: {history_path}")
    logger.info("="*80)


if __name__ == '__main__':
    main()